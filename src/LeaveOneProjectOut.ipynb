{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GerardoFesta/DLforJITDefectPrediction/blob/main/src/LeaveOneProjectOut.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yBm5PLi17kF",
        "outputId": "c06c7271-8877-4cda-caa2-336afa67986e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.10.3-py3-none-any.whl (529 kB)\n",
            "\u001b[K     |████████████████████████████████| 529 kB 27.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.10.3\n"
          ]
        }
      ],
      "source": [
        "from cmath import e\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import sklearn.metrics as skmetrics \n",
        "!pip install torchmetrics\n",
        "from torchmetrics import AUROC\n",
        "import matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BinaryClassificator(nn.Module):\n",
        "  def __init__(self, in_size: int, hidden_size: int, num_classes: int):\n",
        "    super(BinaryClassificator, self).__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(in_size, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "    self.relu = nn.LeakyReLU()\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "\n",
        "    out_1 = self.fc1(x)\n",
        "\n",
        "    out_2 = self.relu(out_1)\n",
        "    out_2_2 = self.relu(out_2)\n",
        "\n",
        "    out_3 = self.fc2(out_2)\n",
        "\n",
        "    return out_3"
      ],
      "metadata": {
        "id": "poI7OT9I2IC5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopper:\n",
        "    def __init__(self, patience=1, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.min_loss = np.inf\n",
        "\n",
        "    def early_stop(self, loss):\n",
        "        if loss < self.min_loss:\n",
        "            self.min_loss = loss\n",
        "            self.counter = 0\n",
        "        elif loss > (self.min_loss + self.min_delta):\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False"
      ],
      "metadata": {
        "id": "8dDZaut7eUd1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class TrainData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n",
        "\n",
        "class TestData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data):\n",
        "        self.X_data = X_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)"
      ],
      "metadata": {
        "id": "QLsi5fUhAbT8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "import numpy as np\n",
        "\n",
        "class CustomScaler(BaseEstimator,TransformerMixin): \n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def fit(self, X, y = None):\n",
        "        self.scaler.fit(X[:, :-1], y)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_head = self.scaler.transform(X[:, 0:-1])\n",
        "        return np.concatenate(X_head, X[:, -1], axis=1)"
      ],
      "metadata": {
        "id": "RvZmSpbNikvi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_pred, y_test):\n",
        "    y_pred_tag = torch.round(torch.sigmoid(y_pred)) #risultati sigmoidali arrotondati\n",
        "    #print(y_pred_tag.shape)\n",
        "    \n",
        "    y_pred_tag = y_pred_tag.to(device)\n",
        "    y_test = y_test.to(device)\n",
        "    correct_results_sum = (y_pred_tag == y_test).sum().float() # num risultati corretti\n",
        "    acc = correct_results_sum/y_test.shape[0] #y_test.spape[0] è il numero di risultati totali\n",
        "    acc = torch.round(acc * 100)\n",
        "    #si fa media e si ritorna accuracy\n",
        "    return acc"
      ],
      "metadata": {
        "id": "dWnJ40nd2_kB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"data_for_analysis.csv\")\n",
        "df = df.drop(columns=[\"TIME\"])\n",
        "\n",
        "\n",
        "device = torch.device('cpu')\n",
        "df['FILE_BUGGY'] = df['FILE_BUGGY'].astype(int)\n",
        "project_list = df['PROJECT'].unique().tolist()\n",
        "\n",
        "X = df.iloc[:, 0:-1] #dataset senza label predizione\n",
        "y = df.iloc[:, -1] #valori da pred.\n",
        "\n",
        "results = pd.DataFrame(columns = [\"Project\", \"Loss\", \"Accuracy\", \"F1\", \"Auc\", \"Precision\", \"Recall\"])\n",
        "print(results)\n",
        "\n",
        "for project in project_list:\n",
        "  train = df[df['PROJECT'] != project]\n",
        "  test = df[df['PROJECT'] == project]\n",
        "  train = train.drop(columns=[\"PROJECT\"])\n",
        "  test = test.drop(columns=[\"PROJECT\"])\n",
        "  X_train = train.iloc[:, 0:-1]\n",
        "  y_train = train.iloc[:, -1]\n",
        "  X_test = test.iloc[:, 0:-1]\n",
        "  y_test = test.iloc[:, -1]\n",
        " \n",
        "  EPOCHS = 50\n",
        "  BATCH_SIZE = 1024\n",
        "  LEARNING_RATE = 0.001\n",
        "\n",
        "  \n",
        "  #Segue soluzione \"one-hot\" per evitare di scalare M_OWN, variabile binaria.\n",
        "  m_own_train = X_train[\"M_OWN\"]\n",
        "  m_own_test = X_test[\"M_OWN\"]\n",
        "  #La salvo per train e test, poi le droppo\n",
        "  X_train = X_train.drop(columns=[\"M_OWN\"])\n",
        "  X_test = X_test.drop(columns=[\"M_OWN\"])\n",
        "  #Lo Standard scaler non cambia l'ordine delle entry, quindi posso aggiungere alla fine M_OWN\n",
        "  \n",
        "      \n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  \n",
        "  m_own_train = m_own_train.to_numpy().reshape([len(m_own_train), 1])\n",
        "  X_train = np.append(X_train, m_own_train, axis=1)\n",
        "\n",
        "  X_test = scaler.transform(X_test)\n",
        "  m_own_test = m_own_test.to_numpy().reshape([len(m_own_test), 1])\n",
        "  X_test = np.append(X_test, m_own_test, axis=1)\n",
        "  \n",
        "  #Crea tensore di training\n",
        "  torch.FloatTensor(X_train)\n",
        "  torch.FloatTensor(y_train.to_numpy())\n",
        "\n",
        "  #Crea tensore di test\n",
        "  train_data = TrainData(torch.FloatTensor(X_train), torch.FloatTensor(y_train.to_numpy()))\n",
        "  test_data = TestData(torch.FloatTensor(X_test))\n",
        "  train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
        "  model = BinaryClassificator(24, 13,  1)\n",
        "  print(model)\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model = model.to(device)\n",
        "  print(device)\n",
        "  model.train()\n",
        "\n",
        "  running_loss = 0.0\n",
        "  running_acc = 0.0\n",
        "  early_stopper = EarlyStopper(patience = 2)\n",
        "  for epoch in range(EPOCHS):  \n",
        "\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        \n",
        "        #manda i batch al device \n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        #azzera gradiente\n",
        "        optimizer.zero_grad()\n",
        "        #predict\n",
        "        y_pred = model(X_batch)\n",
        "        #calcola loss e accuracy\n",
        "\n",
        "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
        "        acc = accuracy(y_pred, y_batch.unsqueeze(1))\n",
        "        #backpropagation della loss\n",
        "        loss.backward()\n",
        "        #ottimizzazione\n",
        "        optimizer.step()\n",
        "        \n",
        "        #somma della loss e dell'accuracy per il batch\n",
        "        running_loss += loss.item()\n",
        "        running_acc += acc.item()                   \n",
        "        \n",
        "    if early_stopper.early_stop(running_loss):             \n",
        "          break\n",
        "    print(f'Epoch {epoch}: | Loss: {running_loss/len(train_loader):.5f} | Acc: {running_acc/len(train_loader):.3f}')\n",
        "  y_pred_list = []\n",
        "  with torch.no_grad():\n",
        "      for X_batch in test_loader:\n",
        "          X_batch = X_batch.to(device)\n",
        "          y_test_pred = model(X_batch)\n",
        "          y_test_pred = torch.sigmoid(y_test_pred)\n",
        "          y_pred_tag = torch.round(y_test_pred)\n",
        "          y_pred_list.append(y_pred_tag.cpu().numpy())\n",
        "\n",
        "  y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
        "  print(classification_report(y_test, y_pred_list))\n",
        "  conf_mat = confusion_matrix(y_test, y_pred_list)\n",
        "  f1_score = skmetrics.f1_score(y_test, y_pred_list)\n",
        "  print(\"SKMetrics F1 score: \")\n",
        "  print(f1_score)\n",
        "\n",
        "  skmetrics.ConfusionMatrixDisplay(conf_mat).plot()\n",
        "  fpr, tpr, _ = skmetrics.roc_curve(y_test, y_pred_list)\n",
        "  roc_display = skmetrics.RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
        "  auc_roc_score = skmetrics.roc_auc_score(y_test, y_pred_list)\n",
        "  prec = skmetrics.precision_score(y_test, y_pred_list)\n",
        "  recall = skmetrics.recall_score(y_test, y_pred_list)\n",
        "  print(\"AUC-ROC score: \")\n",
        "  print(auc_roc_score)\n",
        "  new_row = {\"Project\": project, \"Loss\": running_loss/len(train_loader), \"Accuracy\": running_acc/len(train_loader), \"F1\": f1_score, \"Auc\":auc_roc_score, \"Precision\": prec, \"Recall\": recall}\n",
        "  results = results.append(new_row, ignore_index=True)\n",
        "\n",
        "results.to_csv(\"results.csv\", index = False)\n",
        "from google.colab import files\n",
        "files.download('results.csv') "
      ],
      "metadata": {
        "id": "Nqu4CRdO3NOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nuova sezione"
      ],
      "metadata": {
        "id": "gHUONiWzF3hS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt = matplotlib.pyplot.boxplot(results[\"F1\"], notch=None, vert=None, patch_artist=None, widths=None)\n",
        "\n"
      ],
      "metadata": {
        "id": "2y-si3ACM3b5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}